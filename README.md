ğŸ“˜ Task 5 â€“ Decision Trees & Random Forests
ğŸ“Œ Overview

This project explores tree-based machine learning models to understand how Decision Trees and Random Forests work.
The goal is to train, visualize, evaluate, and compare models while learning key ML concepts like overfitting, tree depth, and feature importance.

Dataset Used: Heart Disease Dataset (as suggested in the task PDF) 

task 5

ğŸ› ï¸ Tools & Libraries

Python

Pandas

NumPy

Scikit-Learn

Matplotlib

Seaborn

Graphviz (optional for tree visualization)

ğŸ“Š Steps Completed

Loaded and explored the Heart Disease dataset

Trained a Decision Tree Classifier

Visualized the tree structure

Performed overfitting analysis using max_depth

Trained a Random Forest Classifier

Compared accuracy with the Decision Tree

Extracted and plotted feature importance

Evaluated model performance using cross-validation

ğŸ” Key Insights

Decision Trees tend to overfit when depth is too high

Limiting the tree depth improves generalization

Random Forest gives higher accuracy and handles overfitting better

Feature importance reveals which medical factors impact predictions most

Cross-validation gives more reliable evaluation scores

â–¶ï¸ How to Run (Google Colab)

Open Google Colab

Upload heart.csv

Upload Task5.ipynb

Run all cells in order

View accuracy, confusion matrix, tree visualization, and feature importance

ğŸ“ Project Structure

task5-decision-trees-rf/
â”‚
â”œâ”€â”€ heart.csv

â”œâ”€â”€ Task5.ipynb

â””â”€â”€ README.md

âœ… Conclusion

Decision Trees are simple and easy to interpret, but Random Forests provide better accuracy and stability.
This task builds understanding of decision boundaries, ensemble learning, and model evaluation.
